---
title: "Lab_2"
output: html_document
date: '2022-06-13'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(tidyverse) # This includes dplyr, stringr, ggplot2, .. 
library(data.table)
library(ggthemes)
library(stringr)
library(tidytext) 
library(rvest)
scipen=50
``` 
Reading the HTML as text
```{r, include=FALSE}
url <- 'https://www.gutenberg.org/files/2701/2701-h/2701-h.htm'
webpage <- read_html(url)
mobybook = html_text(webpage)

```
***1***
***a*** Creating the first paragraph, and then extracting the first line out of it. 
```{r}

book <- webpage %>% html_nodes("div") %>% html_text()
book[1]


```


***b*** Splitting the text string into words, separated by spaces, commas (`,`), periods (`.`), and new line characters (`\n` and `\r`). And printing how many words there are.
```{r}
#lower_text <- str_to_lower(book, locale = "en")
all_words_html <- webpage %>% html_text() %>% str_split(",|\\.|[:space:]")
all_words_html <- all_words_html[[1]]
words <- all_words_html[all_words_html!=""]
print(length(words))

```
Compute the distribution of lengths of words you got, and plot using a bar-plot and computing the `median`, `mean`, `longest` and `most common` word lengths.

```{r}

len_words <- str_length(words)
words_dist<- table(len_words)/length(words)
barplot(words_dist, main = "Distribution of word lenghts", xlab = "Lenght of word", ylab = "Distribution of word length")

median(len_words)
mean(len_words)
max(len_words)

#most common

```

***C*** Count the words frequencies in the text - i.e. the number of times each unique word appears in the text.
Show the top 10 most frequent words with their frequencies. Is the list of top words surprising? explain. 

```{r}

sort(table(words), decreasing = TRUE)[1:10] 

```

It makes sense that these are the 10 most frequent words because they are the most common words in english - conjunction words.


***2***

***a***

```{r}
webpage <- read_html("https://www.gutenberg.org/files/2701/2701-h/2701-h.htm#link2HCH0004")
moby_lines <- html_text2(html_nodes(webpage, 'title,p,h1,h2,h3'))
mobybook <- paste(moby_lines, collapse = " ")
length(mobybook)

# add ETYMOLOGY

chapters = strsplit(mobybook,"\r \r \r \r \r ")
chapters = chapters[[1]][2:137]
```
***B***

```{r}

chapters = strsplit(mobybook,"\r \r \r \r \r ")
chapters = chapters[[1]][2:137]

#chapters
#length(chapters)


    
    
#Count number of elements equal to certain value
    
word_frequencies = function(word,array){
  freq_vec = c()
  for (i in array){ 
    all_words_c <- chapters[i] %>% str_split(",|\\.|[:space:]")
    all_words_c <- all_words_c[[1]]
    words_c <- all_words_c[all_words_c!=""]
    total_words_chap = length(words_c)
    word_freq = sort(table(words_c), decreasing = TRUE)
    our_word = word_freq[word]
    if (is.na(our_word)){
      freq_vec = c(freq_vec,0)}
    else {
      freq_vec = c(freq_vec,our_word/total_words_chap)}
    
    }
  return(freq_vec) 
}
array = c(1:137)
word = "Ahab"
word_frequencies(word,array)

#ggplot(data = mobybook,aes(x=c(0:137)))

word_frequencies("Moby",array)

word_frequencies("sea",chapters)

```
***3***
***A***
***(i)-Formula***
$$P_r(X_A=\omega_i, X_B=\omega_i)= P_r(X_A=\omega_1, X_B=\omega_1)\cup P_r(X_A=\omega_2, X_B=\omega_2)...\cup P_r(X_A=\omega_n, X_B=\omega_n)$$

$*: P(A\cup B)=P(A)+P(B)-P(A\cap B)$, because the intersection is 0, we can rewrite it as: $P(A\cup B)=P(A)+P(B)$. Hence, 
$$P_r(X_A=\omega_i, X_B=\omega_i)= P_r(X_A=\omega_1, X_B=\omega_1)+P_r(X_A=\omega_2, X_B=\omega_2)...+P_r(X_A=\omega_n, X_B=\omega_n)$$
$$\Longrightarrow P_r(X_A=\omega_i, X_B=\omega_i)=\sum_{i=1}^nP_r(X_A=\omega_i, X_B=\omega_i)$$
***(ii)-Simulation***






***B***





***4***
***A***
```{r}

mobybook_clean = str_replace_all(mobybook, "[^a-zA-Z]", " ")
mobybook_clean = tolower(mobybook_clean)
#All words in mobybook
mobybook_split = strsplit(mobybook_clean, " ")
count_letters <- nchar(mobybook_split[[1]] )
#list of all five-letter with duplicates 
five_letters <- mobybook_split[[1]][count_letters ==5]
five_unique = unique(five_letters)
#top 10 most frequent five-letter words with their frequencies
sort(table(five_letters), decreasing = TRUE)[1:10]
```
***B***
```{r}
letter_freq <- table(let = unlist(strsplit(five_letters,'')),pos = sequence(nchar(five_letters)))
heatmap(letter_freq, Colv = NA, Rowv = NA, scale = "column", xlab = "column", ylab = "letter", main = "Letters Heatmap")
```
We can see that for the- 
first location:w
second location:h
third location:a
fourth location:e
fifth location:e
Do you see a strong effect for the location?????????????

***C***

```{r}
#Creating table of probability's according to each letter
alphabet <- letters[1:26]
letter_freq_un <- table(let = unlist(strsplit(unique(five_letters),'')),pos = sequence(nchar(unique(five_letters))))
letter_freq_un2 = letter_freq_un/(length(unique(five_letters)))

#help function in order to multiply a vector
multiply <- function(vec){
    out <- 1
    for(i in 1:length(vec)){
         out <- out*vec[i]
    }
    out
}


probabilities = function (tablegiven,array) {
  vec_prob_let <- c()
  final_pro <- c()
  for (word in array){
    for (j in 1:5){
      letter = substr(word,j,j)
      #print(letter)
      index_row<- match(letter,alphabet)
      #print(index_row)
      pij<- letter_freq_un2[index_row,j]
      #print(pij)
      vec_prob_let<- c(vec_prob_let,pij)
    #print(vec_prob_let)
#print(probability)
#print(final_pro)
    } 
  probability <- multiply(vec_prob_let)
  final_pro <- c(final_pro,probability)

  }
freq_prb_table = as.table(setNames(final_pro,array))


return (freq_prb_table)

}


#word_array = c("where","there")
#frequency_five = probabilities(letter_freq_un2,word_array)

frequency_five = probabilities(letter_freq_un2,unique(five_letters))
sort(frequency_five, decreasing = TRUE)[1:10]


```

***5***
***a***

```{r}
url2 = "https://www-cs-faculty.stanford.edu/~knuth/sgb-words.txt"
common <- read_html(url2,  encoding = "UTF-8")
common_words <- html_text2(common)
common_words <- strsplit(common_words ,split = "(\\s)")
common_words[[1]][1]
  
  
list_string = function(list){
  vec_string <- c()
  for (i in 1:length(common_words[[1]])){
    vec_string <-c(vec_string,common_words[[1]][i])}
  
  return(vec_string)
}

words_string = list_string(common_words) 


letter_freq_dict <- table(let = unlist(strsplit(words_string,'')),pos = sequence(nchar(words_string)))

heatmap(letter_freq_dict, Colv = NA, Rowv = NA, scale = "column", xlab = "column", ylab = "letter", main = "Letters Heatmap")

```
We can see that for the- 
first location:s
second location:a
third location:a
fourth location:e
fifth location:s

compared to:
first location:w
second location:h
third location:a
fourth location:e
fifth location:e

***B***
```{r}

matching = c(c(-1, 1, 1, 0, 0),c(0, 1, 0, 0, 0))
geuss = c("south", "north")
dictionary <- common_words



 #help function for dictionary
data_dict2 = function(dictionary){
  df <- data.frame(dictionary)
  df[c(1:5)]<- str_split_fixed(df$dictionary,"",5)
  df <- df[c(1:5)]
  return(df)
}

dictionary = data_dict2(geuss)
#new_dictionary <- subset(blah,blah[1] == "b")
#new_dictionary <- subset(blah, dictionary!="b" & V2!="b"& V3!="b" & V4!="b" & V5!="b")
#new_dictionary <- subset(blah, dictionary!="t" )

Wordle = function(geuss,matching,dictionary){
  for (i in 1:length(geuss)){
    for (j in 1:5){
      
      if (matching[[i]][j] == -1 ){
        geuss_word <- as.vector(str_split_fixed(geuss[i], pattern = "", n = nchar(geuss[i])))
        letter <- geuss_word[j]
        new_dictionary <- subset(df,df[j]!=letter)
        } else if (matching[[i]][j] == 0){
        geuss_word <- as.vector(str_split_fixed(geuss[i], pattern = "", n = nchar(geuss[i])))
        letter <- geuss_word[j]
        new_dictionary <-       subset(new_dictionary,dictionary!=letter,V2!=letter,V3!=letter,V4!=letter,V5!=letter)
        } else if (matching[[i]][j] == 1){
        geuss_word <- as.vector(str_split_fixed(geuss[i], pattern = "", n = nchar(geuss[i])))
        letter <- geuss_word[j]
        new_dictionary <- subset(df,df[j]==letter)
        }
    }
  }
}
    
check = Wordle(geuss,matching,dictionary)
    
    

```
***6***
***A***
```{r}
```
