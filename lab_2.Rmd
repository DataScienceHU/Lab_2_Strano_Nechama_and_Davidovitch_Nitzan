---
title: "Lab_2"
output: html_document
date: '2022-06-13'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(tidyverse) # This includes dplyr, stringr, ggplot2, .. 
library(data.table)
library(ggthemes)
library(stringr)
library(tidytext) 
library(rvest)
options(scipen=alpha)

``` 
Reading the HTML as text
```{r, include=FALSE}
url <- 'https://www.gutenberg.org/files/2701/2701-h/2701-h.htm'
webpage <- read_html(url)
mobybook = html_text(webpage)

```
***1***
***a***
Load the complete `Moby dick`  book from the [Gutenberg project](https://www.gutenberg.org) into `R`. The book is available [here](https://www.gutenberg.org/files/2701/2701-h/2701-h.htm).
Extract the text from the html as a long string, and print the first line of the text in the file (starting with `The Project Gutenberg ...`)
```{r}

book <- webpage %>% html_nodes("div") %>% html_text()
book[1]


```

***b*** 
Split the text string into words, separated by spaces, commas (`,`), periods (`.`), and new line characters (`\n` and `\r`). How many words are there? 
Compute the distribution of lengths of words you got.

```{r}
#lower_text <- str_to_lower(book, locale = "en")
all_words_html <- webpage %>% html_text() %>% str_split(",|\\.|[:space:]")
all_words_html <- all_words_html[[1]]
words <- all_words_html[all_words_html!=""]
print(length(words))

```
Computing the distribution of lengths of words you got, and plot using a bar-plot and computing the `median`, `mean`, `longest` and `most common` word lengths.

```{r}

len_words <- str_length(words)
words_dist<- table(len_words)/length(words)
barplot(words_dist, main = "Distribution of word lenghts", xlab = "Lenght of word", ylab = "Distribution of word length")

#median
median(len_words)
#mean
mean(len_words)
#longest
max(len_words)
#most common
sort(table(words), decreasing = TRUE)[1]

```

***C***
Count the words frequencies in the text - i.e. the number of times each unique word appears in the text.
Show the top 10 most frequent words with their frequencies. Is the list of top words surprising? explain. 

```{r}
sort(table(words), decreasing = TRUE)
sort(table(words), decreasing = TRUE)[1:10] 

```

It makes sense that these are the 10 most frequent words because they are the most common words in English -  the conjunction words.And also they have the vouls in them which are more common then other letters.


***2***
***a***
Split the book text into `chapters`, such that you obtain an array of strings, one per chapter. 
Count and plot the number of words per each chapter (y-axis) vs. the chapter index (1,2,3.., on x-axis). each chapter is splitted to word in the same manner as in Qu. 1

```{r}
webpage <- read_html("https://www.gutenberg.org/files/2701/2701-h/2701-h.htm#link2HCH0004")
moby_lines <- html_text2(html_nodes(webpage, 'title,p,h1,h2,h3'))
mobybook <- paste(moby_lines, collapse = " ")


chapters = strsplit(mobybook,"\r \r \r \r \r ")

#etymology = 
chapters[[1]][1] <-  sub(".*\r \r ETYMOLOGY.\r \r","",chapters[[1]][1])
chapters = chapters[[1]][1:137]


#Counting words per chapter
word_amount = function(chapters){
  vec_words=vector(length=length(chapters))
  for (i in 1:length(chapters)){
    all_words_c <- chapters[i] %>% str_split(",|\\.|[:space:]")
    words <- all_words_c[all_words_c!=""]
    vec_words[i] = length(all_words_c[[1]])
  }
return(vec_words)
}

#X-is vector
num_word <- word_amount(chapters)
chapter_index = c(1:137)
#chapter_index[1] = "Eti"
#chapter_index[137] = "Epi"

data.frame(num_word) %>% # plotting
ggplot(aes(x=chapter_index, y=num_word)) +
  geom_point() +
  geom_segment( aes(x=c(1:137), xend=c(1:137), y=0, yend=num_word))+
  labs(x="Chapter Number",y= "Words Count Per Chapter",fill=c(1:137),title = "Words Count Per Chapter")



```

***b***
Write a function that receives as input a query word, and an array of strings representing the chapters. The function returns a vector of the `relative frequencies` of the word in each chapter.
Apply the function to the following words `Ahab`, `Moby`, `sea`. Plot for each one of them the trend, i.e. the frequency vs. chapter, with chapters in increasing orders. Do you see a different behavior for the different words? in which parts of the book are they frequent? 


```{r}

word_frequencies = function(word,chapters){
  freq_vec = c()
  for (i in 1:length(chapters)){ 
    all_words_c <- chapters[i] %>% str_split(",|\\.|[:space:]")
    all_words_c <- all_words_c[[1]]
    words_c <- all_words_c[all_words_c!=""]
    total_words_chap = length(words_c)
    word_freq = sort(table(words_c), decreasing = TRUE)
    our_word = as.vector(word_freq[word])
    if (is.na(our_word)){
      freq_vec = c(freq_vec,0)}
    else {
      freq_vec = c(freq_vec,our_word/total_words_chap)}
    
    }
  return(freq_vec) 
}

Ahab_freq <- word_frequencies("Ahab",chapters)
Moby_freq <- word_frequencies("Moby",chapters)
Sea_freq <- word_frequencies("sea",chapters)


data.frame(Ahab_freq) %>% # plotting
ggplot(aes(x=chapter_index, y=Ahab_freq)) +
  geom_point() +
  geom_segment( aes(x=c(1:137), xend=c(1:137), y=0, yend=Ahab_freq))+
  labs(x="Chapter Number",y= "Words Frequency Per Chapter",fill=c(1:137),title = "Words Count Per Chapter For Ahab")

data.frame(Moby_freq) %>% # plotting
ggplot(aes(x=chapter_index, y=Moby_freq)) +
  geom_point() +
  geom_segment( aes(x=c(1:137), xend=c(1:137), y=0, yend=Moby_freq))+
  labs(x="Chapter Number",y= "Words Frequency Per Chapter",fill=c(1:137),title = "Words Count Per Chapter For Moby")

data.frame(Sea_freq) %>% # plotting
ggplot(aes(x=chapter_index, y=Sea_freq)) +
  geom_point() +
  geom_segment( aes(x=c(1:137), xend=c(1:137), y=0, yend=Sea_freq))+
  labs(x="Chapter Number",y= "Words Frequency Per Chapter",fill=c(1:137),title = "Words Count Per Chapter For sea")


```
The frequency is  different between the word's because each word appears in differnent part's of the book at high and low frequency's.
We can see that the word:
"Ahab" appears mostly in the middle of the book and towards the end
"Moby" appears in the middle at high frequenct's and at the end at low frequency's
"Sea" seams to apppear through out the whole book at a rather steady frequency asside from a few chapter's who's frequency is high.


***3***
Suppose that Alice and Bob each choose independently and uniformly at random a single word from the book. That is, each of them chooses a random word instance from the book, taking into account that words have different frequencies (i.e. if for example the word `the` appears $1000$ times, and the word `Ishmael` appears only once, then it is $1000$-times more likely to choose the word `the` because each of its instances can be chosen). What is the probability that they will pick the same word? 
Answer in two ways:
***a***
***(i)-Formula***
Derive an exact formula for this probability as a function of the words relative frequencies, and compute the resulting value for the word frequencies you got for the book.
Because the chance of 2 people  choosing the same word is iid the chance is the multiplication of the distribution of the two words (which is the same)
$$P_r(X_A=\omega_i, X_B=\omega_i)= P_r(X_A=\omega_1) P_r(X_B=\omega_1) = {P_r(X=\omega_i)}^2 $$
In other words:
$${(frequency(word)/n)}^2$$
***Function - Formula***
```{r}
all_word_freq = table(words)
#this function calculates the probability of choosing the same word with a given frequency table
formula_function = function(all_word_freq){
    all_word_freq <-((all_word_freq)/length(words))^2
  #The probability of choosing the same word from the book:
  return(sum(all_word_freq))
}

formula_function(all_word_freq)
```

***(ii) - Simulation***
Simulate $B=100,000$ times the choice of Alice and Bob and use these simulations to estimate the probability that they chose the same word. <br>
```{r}
set.seed(1)
simulation_func = function(words){
  B <- 100000
  counter = 0
  for (i in 1:B){
      sample_words <- sample(words,2,replace = TRUE)
    if(sample_words[1] == sample_words[2]){
      counter = counter + 1
  }
  }  
  return(counter/B)
}
simulation_func(words) 
```
Explain your calculations in both ways and compare the results. Are they close to each other? 
The results are extremely close to each other:
(i) 0.008533267 compared to  (ii) 0.00852.

***b***
Suppose that instead, we took all **unique** words that appear in the book, and then Alice and Bob would choose each independently and uniformly at random a single word from the list of unique words. What would be the probability that they chose the same word in this case? is it lower, the same, or higher then the probability in (a.)? explain why.

The number of unique words is n, the probability that Alice and Bob will choose the same specific word is $$1/n∗1/n=1/n^2$$
Because we have n words, the probability that Alice and Bob will chose the same word is $$n∗1/n^2=1/n$$


```{r}
unique_words = unique(words)
1/length(unique_words)
```
$$0.00003811993$$ is significantly smaller then the probability we got in (a)
This is possible and makes sense since in (a) the probability was $${(frequency(word)/n)}^2$$ which is at least $$1/n^2$$


***4***!!! what about ""

***a***
Extract from the book a list of all `five-letter` words. Keep only words that have only English letters. Convert all to lower-case. How many words are you left with? how many unique words? 
Show the top 10 most frequent five-letter words with their frequencies.
```{r}

mobybook_clean = str_replace_all(mobybook, "[^a-zA-Z]", " ")
mobybook_clean = tolower(mobybook_clean)

#All words in mobybook
mobybook_split = strsplit(mobybook_clean, " ")
count_letters <- nchar(mobybook_split[[1]] )
#list of all five-letter with duplicates 
five_letters <- mobybook_split[[1]][count_letters ==5]

#amount of 5 letter words in the book:
print(length(five_letters))

five_unique = unique(five_letters)
#amount of 5 unique letter words in the book:
print(length(five_unique)) 

#top 10 most frequent five-letter words with their frequencies
sort(table(five_letters), decreasing = TRUE)[1:10]
```
We are left with:
* 5 letter words in the book -26513
* 5 unique letter words in the book -1980
* top 10 most frequent five-letter words with their frequencies-
whale there which their would other these still about great 
 1226   861   640   616   432   417   403   312   310   307 


***b***
Compute letter frequencies statistics of the five-letter words: 
That is, for each of the five locations in the word (first, second,..), how many times each of the English letters `a`, `b`,...,`z` appeared in your (non-unique) list of words. Store the result in a `26-by-5` table and show it as a heat map. Which letter is most common in each location? Do you see a strong effect for the location?
```{r}
letter_freq <- table(let = unlist(strsplit(five_letters,'')),pos = sequence(nchar(five_letters)))
heatmap(letter_freq, Colv = NA, Rowv = NA, scale = "column", xlab = "column", ylab = "letter", main = "Letters Heatmap")
```
We can see that for the- 
first location:w
second location:h
third location:a
fourth location:e
fifth location:e
We can see that the fifth and fourth and third position the voul's appear more often, this is expected since those letter's appear many times in many word's.


***c***
Consider the following random model for typing words: we have a `26-by-5` table of probabilities $$p_{ij}$$ for i from $$1$$ to $$5$$, 
and $j$ going over all $26$ possible English letters (assuming lower-case). (This table stores the parameters of the model).
Here, $$p_{ij}$$ is the probability that the $i$-th letter in the word will be the character $j$. 
Now, each letter $i$ is typed from a categorical distribution over the $26$ letters, with probability $p_{ij}$ of being the character $j$, and the letters are drawn independently for different values of $i$. 
For example,  using $$p_{5s}=0.3$$ we will draw words such that the probability of the last character being `s` will be $0.3$. <br>
For each five-letter word $$w$$, its likelihood under this model is defined simply as the probability of observing this word when drawing a word according to this model, that is, if $$w=(w_1,w_2,w_3,w_4,w_5)$$ with $$w_i$$ denoting the $i$-th letter, then $$Like(w ; p) = \prod_{i=1}^5 p_{i w_i}$$. <br>
Write a function that receives a `26-by-5` table of probabilities and an array of words (strings), and computes the likelihood of each word according to this model. <br>
Run the function to compute the likelihood of all unique five-letter words from the book, with the frequency table you computed in 4.b. normalized to probabilities. Show the top-10 words with the highest likelihood. 

```{r}
#Creating table of probability's according to each letter
alphabet <- letters[1:26]
letter_freq_un <- table(let = unlist(strsplit(unique(five_letters),'')),pos = sequence(nchar(unique(five_letters))))
letter_freq_un2 = letter_freq_un/(length(unique(five_letters)))

#help function in order to multiply a vector
multiply <- function(vec){
    out <- 1
    for(i in 1:length(vec)){
         out <- out*vec[i]
    }
    out
}

probabilities = function(tablegiven,array) {
  vec_prob_let <- c()
  final_pro <- c()
  for (word in array){
    for (j in 1:5){
      letter = substr(word,j,j)
      #print(letter)
      index_row<- match(letter,alphabet)
      #print(index_row)
      pij<- letter_freq_un2[index_row,j]
      #print(pij)
      vec_prob_let<- c(vec_prob_let,pij)
    #print(vec_prob_let)
#print(probability)
#print(final_pro)
    } 
  probability <- multiply(vec_prob_let)
  final_pro <- c(final_pro,probability)

  }
freq_prb_table = as.table(setNames(final_pro,array))
return (freq_prb_table)

}


#word_array = c("where","there")
#frequency_five = probabilities(letter_freq_un2,word_array)

frequency_five = probabilities(letter_freq_un2,unique(five_letters))
sort(frequency_five, decreasing = TRUE)[1:10]
letter_freq_un <- table(let = unlist(strsplit(unique(five_letters),'')),pos = sequence(nchar(unique(five_letters))))
letter_freq_un2 = letter_freq_un/(length(unique(five_letters)))



```


***5***
***a***
Download the list of five-letter words from [here](https://www-cs-faculty.stanford.edu/~knuth/sgb-words.txt). This list contains most common English five-letter words (each word appears once).  
Compute and display the `26-by-5` table of frequencies for this word list, in similar to Qu. 4.b.
Do you see major differences between the two tables? why? 

```{r}
common <- read.csv("five-letter-words-knuth.txt")
common <-as.vector(common)

#Converting a list to string
list_string = function(list){
  vec_string <- c()
  for (i in 1:length(common[[1]])){
    vec_string <-c(vec_string,common[[1]][i])}
  
  return(vec_string)
}

words_string = list_string(common) 

letter_freq_dict <- table(let = unlist(strsplit(words_string,'')),pos = sequence(nchar(words_string)))

heatmap(letter_freq_dict, Colv = NA, Rowv = NA, scale = "column", xlab = "column", ylab = "letter", main = "Letters Heatmap")


```
We can see that for the- 
first location:s
second location:a
third location:a
fourth location:e
fifth location:s

compared to question 4:
first location:w
second location:h
third location:a
fourth location:e
fifth location:e

Again the fourth and fifth position's have voul's which is as expected.


***b***
Write a function that receives an array of guess words, an array of their corresponding matches to the unknown word (i.e. a two-dimensional array), and a `disctionary` - i.e. an array of legal English words. 
The function should return all the words in the dictionary that are consistent with the results of the previous guesses. For example, if we guessed `maple` and our match was the array `[1, 0, -1, 0, 0]`, then we should keep only words that start with an `m`, have a `p` at a location different from $3$, and don't have `a`, `l` and `e`.
When we have multiple guesses, our list of consistent words should be consistent with all of them, hence as we add more guesses, the list of consistent words will become shorter and shorter. <br>
Run your function on the list of words from (a.), and with the guesses `c("south", "north")` and their corresponding matches: `c(-1, 1, 1, 0, 0)` and `c(0, 1, 0, 0, 0)`. Output the list of consistent words with these two guesses. 

```{r}
#help function for dictionary- make in to a data frame
data_dict2 = function(dictionary){
  words_vec = dictionary
  df <- data.frame(words_vec)
  df[c(1:5)]<- str_split_fixed(df$words_vec,"",5)
  df <- df[c(1:5)]
  return(df)
}

Wordle = function(geuss,matching,dictionary){
  dictionary = data_dict2(dictionary)
  for (i in 1:length(geuss)){
    for (j in 1:5){
        if (matching[j + (i-1) *5] == 0) {
        geuss_word <- as.vector(str_split_fixed(geuss[i], pattern = "", n = nchar(geuss[i])))
        
        dictionary <- subset(dictionary, (dictionary[1]!=geuss_word[j]) &
                                         (dictionary[2]!=geuss_word[j]) &
                                         (dictionary[3]!=geuss_word[j]) &
                                         (dictionary[4]!=geuss_word[j]) &
                                         (dictionary[5]!=geuss_word[j]))

        } else if (matching[j + (i-1) *5] == -1){
        geuss_word <- as.vector(str_split_fixed(geuss[i], pattern = "", n = nchar(geuss[i])))
                dictionary <- subset(dictionary, dictionary[1]==geuss_word[j] |
                                         dictionary[2]==geuss_word[j] |
                                         dictionary[3]==geuss_word[j] |
                                         dictionary[4]==geuss_word[j] |
                                         dictionary[5]==geuss_word[j])
                
        dictionary <- subset(dictionary,dictionary[j]!=geuss_word[j])

        } else {
        geuss_word <- as.vector(str_split_fixed(geuss[i], pattern = "", n = nchar(geuss[i])))
        dictionary <- subset(dictionary,dictionary[j]==geuss_word[j])
        }
    }
  }
dictionary <- na.omit(dictionary)
df <- c(dictionary, sep = "")
result <- do.call(paste,df)
return(result)
}

geuss = c("south", "north")
check = Wordle(geuss,c(c(-1, 1, 1, 0, 0), c(0, 1, 0, 0, 0)),words_string)
check
```


***6***
***a***
**strategy 1:**
Implement a function that receives as input the unknown word, and implements this strategy. The output should be the number of turns it took to guess the word. The function should also record and print guess at each turn, as well as the match array , until the word is guessed correctly.  
Run the function when the unknown word is "mouse", and show the results.

```{r}

set.seed(1)
strategy_one = function(unknown,tn=TRUE){
  # splitting unknown word into 5 letters
  unknown_word = as.vector(str_split_fixed(unknown, pattern = "", n = nchar(unknown)))
  # guessing 5 random letters
  guesses = c(sample(letters, 1),sample(letters, 1),sample(letters, 1),sample(letters, 1),sample(letters, 1))
    guess_count = 1
  while (!(identical(unknown_word,guesses)==TRUE)){
      guess_array = c()
      guess_count = guess_count + 1
  for (i in 1:5){
      if(isTRUE(unknown_word[i]==guesses[i])==TRUE){
        guess_array[i] = 1 
        }
      else{
        guess_array[i] = 0
        guesses[i] = sample(letters, 1)
      }
    guessed_word = str_c(guesses,collapse="")
    guessed_array = str_c(guess_array,collapse="")
  }
      
if (tn==TRUE){
        print(cbind(guessed_word,guessed_array))

      }  }
  return (guess_count)
}

strategy_one_try = strategy_one("mouse")
  
```

***b***
Write a mathematical formula for the distribution of the number of turns needed to guess the target word with this strategy.


The Geomertrical distribution is:
$$Pr(X=k)=(1-p)^{k-1}p$$
Cumulitive Geomertrical distribution is:
$$Pr(X ≤ x) = 1 - (1 - p)^{x}$$
Because each letter is iid the intersection is the multipication of each letters geometric distribution, there-fore the probability of geussing at or before x is the probability of each letter geussing at less than x or on x. therefore:

$$Pr(X <= x) = Pr(X ≤ x)∩Pr(X ≤ x) ... ∩Pr(X ≤ x) = Pr(X ≤ x)*Pr(X ≤ x) ... *Pr(X ≤ x)$$
By calculating this we will get:
$$Pr(X ≤ x)*Pr(X ≤ x) ... *Pr(X ≤ x) = 1 - (1 - p)^{x} * 1 - (1 - p)^{x} ... 1 - (1 - p)^{x} $$ 
$$1 - (1 - p)^{x} * 1 - (1 - p)^{x}*1 - (1 - p)^{x}* 1 - (1 - p)^{x}*1 - (1 - p)^{x} = {(1 - (1 - p)^{x})}^{n}$$
By the identity we get that:
$$P(X=x)=P(X≤x)−P(X≤x−1)⇒P(X=x)={[{1−(1−p)}^x]}^n−{[{1−(1−p)}^{x-1}]}^n$$
We know the chance of each letter is uniform from 26 letter's, therefore:
$$p=1/26$$
We will continue by calculating:
$$P(X=x)={[{1−(1−1/26)}^x]}^n−{[{1−(1−1/26)}^{x-1}]}^n$$
By definition $$E(x)$$ is:

$$E(x) = \sum_{i=1}^{x=10000}x*Pr(X=x)$$
A function that will calculate the distribution of P(X=x) is-

```{r}
expec_func <- function(x){ 
  p <- 1/26
  n <- 5
  expec = 0
  for (i in 1:10000){
    dist <- (1-(1-p)^i)^n - (1-(1-p)^(i-1))^n
    expectation = dist*i
    expec = expec + expectation}
  print(expec)
}
expectation_X = expec_func(10000)
```


***c***
Compute empirically the distribution of the number of turns using the following Monte-Carlo simulation:
- Draw $B=100$ random unknown words, uniformly at random from the list of five-letter words in Qu. 5. 
- For each unknown word, run the guessing strategy implemented in (a.) and record the number of turns 
- Compute the average number of turns across all $B=100$ simulations. <br>
Plot the empirical CDF along with the theoretical CDF from (b.) on the same plot. Do they match? 
compare also the empirical expectation with the expectation computed in (b.). How close are they? 

```{r}
set.seed(1)
sampled_word = sample(words_string,1)
monte_carlo <-replicate(100,strategy_one(sampled_word,tn=FALSE))
mean(monte_carlo)
plot(ecdf(monte_carlo))
points <- seq(1:100)

#better plot??

```

***Question 7***

***a***
Implement the following two additional strategies for guessing the word: 
**Strategy 2:** 
- At each stage, we guess the word with the highest likelihood (see Qu. 4.c.), **of the remaining words that are consistent with the previous guesses**. 
- We keep guessing until obtaining the correct word. 
4.2
**Strategy 3:** 
The same as strategy 2, but at each stage we guess a random word sampled uniformly from all remaining consistent words (instead of guessing the word with the highest likelihood).
4.8
Run both strategies with the unknown word "mouse", and show the guesses and the number of turns for them, in similar to Qu. 6.a.



```{r}
# Helper function: 
wordle_match <- function(guess, word)  # 1: correct location, -1: wrong location, 0: missing
{
  L <- nchar(guess)
  match <- rep(0, L)
  for(i in 1:L)
  {
    if(grepl(substr(guess, i, i), word, fixed=TRUE))
      {match[i] = -1}
    if(substr(guess, i, i) == substr(word, i, i))
    {      match[i] = 1}
  }
  
  return(match)
}
```

**Strategy 2:** 


```{r}
strat2 = function(word_list, unknown_word,if_print=TRUE){
  unknown_word_2 = as.vector(str_split_fixed(unknown_word, pattern = "", n = nchar(unknown_word)))
  #using 4 in order to find freq of words_string and find the starting word
  wordle_freq <- table(let = unlist(strsplit(word_list,'')),pos = sequence(nchar(word_list)))
wordle_prob = wordle_freq/(length(word_list))
frequency_words = probabilities(wordle_prob,word_list)
picked_word<- sort(frequency_words , decreasing = TRUE)[1]
picked_word <- names(picked_word)

new_df <- word_list
counter = 0 
  while (!identical(picked_word,unknown_word)){
  vec_array = wordle_match(picked_word, unknown_word)
  
#Create new dictionary using 5b
new_df = Wordle(picked_word,vec_array,new_df)

#choosing new word
wordle_freq <- table(let = unlist(strsplit(new_df,'')),pos = sequence(nchar(new_df)))
wordle_prob = wordle_freq/(length(new_df))
frequency_words = probabilities(wordle_prob,new_df)
picked_word<- sort(frequency_words , decreasing = TRUE)[1]
picked_word <- names(picked_word)
counter = counter +1 
if(if_print == TRUE){
  print(picked_word)

}
  }
  print(counter)
}

unknown_word <- "mouse"
a_df <- strat2(words_string, unknown_word,if_print=FALSE)


```


**Strategy 3:** 

```{r}
strat3 = function(word_list, unknown_word,if_print=TRUE){
  picked_word = sample(word_list,1)
  #using 4 in order to find freq of words_string and find the starting word
 
new_df <- word_list
counter = 0 
  while (!identical(picked_word,unknown_word)){
  vec_array = wordle_match(picked_word, unknown_word)
  
#Create new dictionary using 5b
new_df = Wordle(picked_word,vec_array,new_df)

#choosing new word
  picked_word = sample(word_list,1)
  counter = counter +1
  if(if_print == TRUE){
  print(picked_word)

}
  }
  print(counter)
  }

unknown_word <- "mouse"
strat3(words_string, unknown_word,if_print = FALSE)


```
***b***
Run $B = 100$ simulations of the games, in similar to Qu. 6.c. 
That is, each time, sample a random unknown word,  run the two strategies $2$ and $3$, and record the number of turns needed to solve `wordle` for both of them. 

- Plot the empirical CDFs of the number of guesses. How similar are they to each other? how similar are they to the CDF of strategy 1? What are the empirical means for both strategies?  

**Strategy 2 and 3 simulation:** 

```{r}

random_words = sample(words_string,100,replace = TRUE)
strat2_tries=sapply(random_words,function(new_word){strat2(words_string,new_word,if_print=FALSE)})
strat3_tries=sapply(random_words,function(new_word){strat3(words_string,new_word,if_print=FALSE)})

strat2_mean = mean(strat2_tries)
strat3_mean = mean(strat3_tries)

```





