---
title: "Lab_2"
output: html_document
date: '2022-06-13'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(tidyverse) # This includes dplyr, stringr, ggplot2, .. 
library(data.table)
library(ggthemes)
library(stringr)
library(tidytext) 
library(rvest)
``` 
Reading the HTML as text
```{r, include=FALSE}

url <- 'https://www.gutenberg.org/files/2701/2701-h/2701-h.htm'
webpage <- read_html(url)
mobybook = html_text(webpage)

```
***1***
***a*** Creating the first paragraph, and then extracting the first line out of it. 
```{r}

book <- webpage %>% html_nodes("div") %>% html_text()
class(book)
book[1]


```

***b*** Splitting the text string into words, separated by spaces, commas (`,`), periods (`.`), and new line characters (`\n` and `\r`). And printing how many words there are.
```{r}

all_words_html <- webpage %>% html_text() %>% str_split(",|\\.|[:space:]")
all_words_html <- all_words_html[[1]]
words <- all_words_html[all_words_html!=""]
print(length(words))


```
Compute the distribution of lengths of words you got, and plot using a bar-plot and computing the `median`, `mean`, `longest` and `most common` word lengths.
```{r}

len_words <- str_length(words)
words_dist<- table(len_words)/length(words)
barplot(words_dist, main = "Distribution of word lenghts", xlab = "Lenght of word", ylab = "Distribution of word length")

median(len_words)
mean(len_words)
max(len_words)

#most common

```

***C*** Count the words frequencies in the text - i.e. the number of times each unique word appears in the text.
Show the top 10 most frequent words with their frequencies. Is the list of top words surprising? explain. 

```{r}
sort(table(words), decreasing = TRUE)[1]

sort(table(words), decreasing = TRUE)[1:10] 

```

It makes sense that these are the 10 most frequent words because they are the most common words in english - conjunction words.


***2***

***a***

```{r}

strsplit(mobybook,"\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n")

chapters = strsplit(mobybook,"CHAPTER")
chapters[[1]][200]
chapters = chapters[[1]][137:length(chapters[[1]])]
length(chapters)


  #"\r\n    \r\n    \r\n      \r\n       \r\n    \r\n    \r\n      \r\n    \r\n      "
#webpage
#elements <- strsplit(book,split ='\n\n\n\n\n\n')[[1]]
#chapters <- elements[2:length(elements)]


#chapters = webpage %>% html_text()  %>% str_split("\r\n    \r\n    \r\n      \r\n       \r\n    \r\n    \r\n      \r\n    \r\n      ")

#chapters2 = webpage %>% html_text()  %>% str_split("\r\n     \r\n    \r\n      \r\n       \r\n    \r\n    \r\n      \r\n    \r\n      ")

#chapters = webpage %>% html_text()  %>% str_split("\r\n    \r\n    \r\n      \r\n       \r\n    \r\n    \r\n      \r\n    \r\n      ")

#chapters[[1]][14]
#chapters2[[1]][2]

#all_chapters = c(chapters,chapters2)
#all_chapters[[2]]

```




***B***

```{r}

length(chapters)
chapters = chapters

class(mobybook)

word_frequencies = function(word,chapters){
  freq_vec = c()
  for (i in length(array)){ 
    chapter_words <- strsplit(chapters[[1]][i],",|\\.|[:space:]")
    #all_words_html <- all_words_html[[1]]
    chapter_words <- chapter_words[all_words_html!=""]
    total_words = length(chapter_words)
    word_freq = sort(table(words), decreasing = TRUE)
    freq = word_freq[word_freq[,1]==word,2]
    freq_vec = c(freq_vec,freq/total_words)
    
    }
  return(freq_vec) 
}

word_frequencies("Ahab",chapters)
ggplot(data = mobybook,aes(x=c(0:135)))

word_frequencies("Moby",chapters)

word_frequencies("sea",chapters)


```




