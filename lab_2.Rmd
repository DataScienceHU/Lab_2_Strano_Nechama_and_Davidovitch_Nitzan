---
title: "Lab_2"
output: html_document
date: '2022-06-13'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(tidyverse) # This includes dplyr, stringr, ggplot2, .. 
library(data.table)
library(ggthemes)
library(stringr)
library(tidytext) 
library(rvest)
``` 
Reading the HTML as text
```{r, include=FALSE}
url <- 'https://www.gutenberg.org/files/2701/2701-h/2701-h.htm'
webpage <- read_html(url)
mobybook = html_text(webpage)

```
***1***
***a*** Creating the first paragraph, and then extracting the first line out of it. 
```{r}

book <- webpage %>% html_nodes("div") %>% html_text()
class(book)
book[1]


```

***b*** Splitting the text string into words, separated by spaces, commas (`,`), periods (`.`), and new line characters (`\n` and `\r`). And printing how many words there are.
```{r}

all_words_html <- webpage %>% html_text() %>% str_split(",|\\.|[:space:]")
all_words_html <- all_words_html[[1]]
words <- all_words_html[all_words_html!=""]
print(length(words))


  ```
Compute the distribution of lengths of words you got, and plot using a bar-plot and computing the `median`, `mean`, `longest` and `most common` word lengths.
```{r}

len_words <- str_length(words)
words_dist<- table(len_words)/length(words)
barplot(words_dist, main = "Distribution of word lenghts", xlab = "Lenght of word", ylab = "Distribution of word length")

median(len_words)
mean(len_words)
max(len_words)

#most common

```

***C*** Count the words frequencies in the text - i.e. the number of times each unique word appears in the text.
Show the top 10 most frequent words with their frequencies. Is the list of top words surprising? explain. 

```{r}
sort(table(words), decreasing = TRUE)[1]

sort(table(words), decreasing = TRUE)[1:10] 

```

It makes sense that these are the 10 most frequent words because they are the most common words in english - conjunction words.


***2***

***a***

```{r}
webpage <- read_html("https://www.gutenberg.org/files/2701/2701-h/2701-h.htm#link2HCH0004")
moby_lines <- html_text2(html_nodes(webpage, 'title,p,h1,h2,h3'))
mobybook <- paste(moby_lines, collapse = " ")
length(mobybook)

# add ETYMOLOGY

chapters = strsplit(mobybook,"\r \r \r \r \r ")
mobybook[[1]][1]


#chapters = strsplit(mobybook,"CHAPTER")
#chapters[[1]][200]
#chapters = chapters[[1]][137:length(chapters[[1]])]
#length(chapters)


  #"\r\n    \r\n    \r\n      \r\n       \r\n    \r\n    \r\n      \r\n    \r\n      "
#webpage
#elements <- strsplit(book,split ='\n\n\n\n\n\n')[[1]]
#chapters <- elements[2:length(elements)]


#chapters = webpage %>% html_text()  %>% str_split("\r\n    \r\n    \r\n      \r\n       \r\n    \r\n    \r\n      \r\n    \r\n      ")

#chapters2 = webpage %>% html_text()  %>% str_split("\r\n     \r\n    \r\n      \r\n       \r\n    \r\n    \r\n      \r\n    \r\n      ")

#chapters = webpage %>% html_text()  %>% str_split("\r\n    \r\n    \r\n      \r\n       \r\n    \r\n    \r\n      \r\n    \r\n      ")

#chapters[[1]][14]
#chapters2[[1]][2]

#all_chapters = c(chapters,chapters2)
#all_chapters[[2]]

```




***B***

```{r}

length(chapters)
chapters = chapters

class(mobybook)

word_frequencies = function(word,chapters){
  freq_vec = c()
  for (i in length(array)){ 
    chapter_words <- strsplit(chapters[[1]][i],",|\\.|[:space:]")
    #all_words_html <- all_words_html[[1]]
    chapter_words <- chapter_words[all_words_html!=""]
    total_words = length(chapter_words)
    word_freq = sort(table(words), decreasing = TRUE)
    freq = word_freq[word_freq[,1]==word,2]
    freq_vec = c(freq_vec,freq/total_words)
    
    }
  return(freq_vec) 
}

word_frequencies("Ahab",chapters)
ggplot(data = mobybook,aes(x=c(0:135)))

word_frequencies("Moby",chapters)

word_frequencies("sea",chapters)


```

***4***
***a***

```{r}
mobybook
#mobybook1 <- sub("[0-9.]+$", "", mobybook )

#mobybook <- gsub("[[:punct:]]*span[[:punct:]]*", "", mobybook) # Remove 

class(chapters)
mobybook1 = strsplit(mobybook,split="\r|\n| |\\.|\\.|\\!|\\;|\\,")
mobybook1 = mobybook1[mobybook1 != ""]

#mobybook2 = gsub('[\r]'," ",mobybook1)

#mobybook1

#mobybook = gsub('\\s?(\\d+|\\.)|[\r]|[\n]|,|-|!|;| -', '', mobybook)
#mobybook = gsub('[\r]', '', mobybook)


#mobybook = gsub("\\s+#.*$-", "", mobybook)
#mobybook1<- sub("\\.", "", mobybook)


#class(v)

```



***5***
***a***

```{r}
url2 = "https://www-cs-faculty.stanford.edu/~knuth/sgb-words.txt"
common = read.table(url2)

sort(table(common), decreasing = TRUE)

```






