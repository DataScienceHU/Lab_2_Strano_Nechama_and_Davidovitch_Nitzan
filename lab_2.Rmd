---
title: "Lab_2"
output: html_document
date: '2022-06-13'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(tidyverse) # This includes dplyr, stringr, ggplot2, .. 
library(data.table)
library(ggthemes)
library(stringr)
library(tidytext) 
library(rvest)
scipen=50
``` 
Reading the HTML as text
```{r, include=FALSE}
url <- 'https://www.gutenberg.org/files/2701/2701-h/2701-h.htm'
webpage <- read_html(url)
mobybook = html_text(webpage)

```
***1***
***a*** Creating the first paragraph, and then extracting the first line out of it. 
```{r}

book <- webpage %>% html_nodes("div") %>% html_text()
class(book)
book[1]


```


***b*** Splitting the text string into words, separated by spaces, commas (`,`), periods (`.`), and new line characters (`\n` and `\r`). And printing how many words there are.
```{r}
#lower_text <- str_to_lower(book, locale = "en")
all_words_html <- webpage %>% html_text() %>% str_split(",|\\.|[:space:]")
all_words_html <- all_words_html[[1]]
words <- all_words_html[all_words_html!=""]
print(length(words))

```
Compute the distribution of lengths of words you got, and plot using a bar-plot and computing the `median`, `mean`, `longest` and `most common` word lengths.

```{r}

len_words <- str_length(words)
words_dist<- table(len_words)/length(words)
barplot(words_dist, main = "Distribution of word lenghts", xlab = "Lenght of word", ylab = "Distribution of word length")

median(len_words)
mean(len_words)
max(len_words)

#most common

```

***C*** Count the words frequencies in the text - i.e. the number of times each unique word appears in the text.
Show the top 10 most frequent words with their frequencies. Is the list of top words surprising? explain. 

```{r}
sort(table(words), decreasing = TRUE)[1]

sort(table(words), decreasing = TRUE)[1:10] 

```

It makes sense that these are the 10 most frequent words because they are the most common words in english - conjunction words.


***2***

***a***

```{r}
webpage <- read_html("https://www.gutenberg.org/files/2701/2701-h/2701-h.htm#link2HCH0004")
moby_lines <- html_text2(html_nodes(webpage, 'title,p,h1,h2,h3'))
mobybook <- paste(moby_lines, collapse = " ")
length(mobybook)

# add ETYMOLOGY

chapters = strsplit(mobybook,"\r \r \r \r \r ")
chapters = chapters[[1]][2:137]
chapters
```
***B***

```{r}

chapters = strsplit(mobybook,"\r \r \r \r \r ")
chapters = chapters[[1]][2:137]

#chapters
#length(chapters)


freq_vec = c()
    all_words_c <- chapters[1] %>% str_split(",|\\.|[:space:]")
    all_words_c <- all_words_c[[1]]
    words_c <- all_words_c[all_words_c!=""]
    total_words = length(words_c)
    word_freq = sort(table(words_c), decreasing = TRUE)
    our_word = word_freq["Ahab"]

    freq_vec = c(freq_vec,word_freq/total_words)
    


word_frequencies = function(word,array){
  freq_vec = c()
  for (i in length(array)){ 
    all_words_c <- chapters[i] %>% str_split(",|\\.|[:space:]")
    all_words_c <- all_words_c[[1]]
    words_c <- all_words_c[all_words_c!=""]
    total_words_chap = length(words_c)
    word_freq = sort(table(words_c), decreasing = TRUE)
    our_word = word_freq[word]
    freq_vec = c(freq_vec,our_word/total_words_chap)
    }
  return(freq_vec) 
}
array = c(1:137)
word_frequencies("Ahab",array)
ggplot(data = mobybook,aes(x=c(0:137)))

word_frequencies("Moby",chapters)

word_frequencies("sea",chapters)

```
***4***
***A***
```{r}

mobybook_clean = str_replace_all(mobybook, "[^a-zA-Z]", " ")
mobybook_clean = tolower(mobybook_clean)
#All words in mobybook
mobybook_split = strsplit(mobybook_clean, " ")
count_letters <- nchar(mobybook_split[[1]] )
#list of all five-letter with duplicates 
five_letters <- mobybook_split[[1]][count_letters ==5]
length(five_letters)
five_unique = unique(five_letters)
length(five_unique)
#top 10 most frequent five-letter words with their frequencies
sort(table(five_letters), decreasing = TRUE)[1:10]
```
***B***
```{r}
alphabet <- letters[1:26]
letter_freq <- table(let = unlist(strsplit(five_letters,'')),pos = sequence(nchar(five_letters)))

heatmap(letter_freq, Colv = NA, Rowv = NA, scale = "column", xlab = "column", ylab = "letter", main = "Letters Heatmap")
```
We can see that for the- 
first location:w
second location:h
third location:a
fourth location:e
fifth location:e
Do you see a strong effect for the location?????????????

***C***

```{r}

```

***5***
***a***

```{r}
url2 = "https://www-cs-faculty.stanford.edu/~knuth/sgb-words.txt"
common = read.table(url2)

sort(table(common), decreasing = TRUE)


```{r}
#creating a table with 5 columns named 1-5
#making a list of letters where each
#Japan Population Data Frame - Step 1    
#Japan.Population <- data.frame(11, "Japan", 126860301,    127202192, "-0.27%")      
  
#Naming the Data Frame - Step 2  
#names(Japan.Population) <- c("Rank", "Country", "Population.2019", "Population.2018", "Growth.Rate")  
  
#Using rbind() function to insert above observation  
#WorldPopulation.Newdf <- rbind(DataFrame.WorldPopulation, Japan.Population)
```

