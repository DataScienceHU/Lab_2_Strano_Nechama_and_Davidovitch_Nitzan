word_freq = sort(table(words_c), decreasing = TRUE)
our_word = as.vector(word_freq[word])
if (is.na(our_word)){
freq_vec = c(freq_vec,0)}
else {
freq_vec = c(freq_vec,our_word/total_words_chap)}
}
return(freq_vec)
}
word_frequencies("Ahab",chapters)
word_frequencies("Moby",chapters)
word_frequencies("sea",chapters)
#add ggplot of the three words freq = y and x =chapter number
length(chapters)
length(chapters[[1]])
length(chapters[[1]])
word_frequencies = function(word,chapters){
freq_vec = c()
for (i in 1:length(chapters[[1]])){
all_words_c <- chapters[i] %>% str_split(",|\\.|[:space:]")
all_words_c <- all_words_c[[1]]
words_c <- all_words_c[all_words_c!=""]
total_words_chap = length(words_c)
word_freq = sort(table(words_c), decreasing = TRUE)
our_word = as.vector(word_freq[word])
if (is.na(our_word)){
freq_vec = c(freq_vec,0)}
else {
freq_vec = c(freq_vec,our_word/total_words_chap)}
}
return(freq_vec)
}
word_frequencies("Ahab",chapters)
word_frequencies("Moby",chapters)
word_frequencies("sea",chapters)
#add ggplot of the three words freq = y and x =chapter number
chapters = chapters[[1]][1:137]
chapters = chapters[[1]][1:137]
word_frequencies = function(word,chapters){
freq_vec = c()
for (i in 1:length(chapters)){
all_words_c <- chapters[i] %>% str_split(",|\\.|[:space:]")
all_words_c <- all_words_c[[1]]
words_c <- all_words_c[all_words_c!=""]
total_words_chap = length(words_c)
word_freq = sort(table(words_c), decreasing = TRUE)
our_word = as.vector(word_freq[word])
if (is.na(our_word)){
freq_vec = c(freq_vec,0)}
else {
freq_vec = c(freq_vec,our_word/total_words_chap)}
}
return(freq_vec)
}
word_frequencies("Ahab",chapters)
word_frequencies("Moby",chapters)
word_frequencies("sea",chapters)
#add ggplot of the three words freq = y and x =chapter number
chapters[1]
chapters[2]
chapters[[1]][2]
chapters = strsplit(mobybook,"\r \r \r \r \r ")
#etymology =
chapters[[1]][1] <-  sub(".*\r \r ETYMOLOGY.\r \r","",chapters[[1]][1])
chapters = chapters[[1]][1:137]
chapters[1]
chapters[2]
chapters[3]
chapters = chapters[[1]][1:137]
webpage <- read_html("https://www.gutenberg.org/files/2701/2701-h/2701-h.htm#link2HCH0004")
moby_lines <- html_text2(html_nodes(webpage, 'title,p,h1,h2,h3'))
mobybook <- paste(moby_lines, collapse = " ")
# add ETYMOLOGY
chapters = strsplit(mobybook,"\r \r \r \r \r ")
#etymology =
chapters[[1]][1] <-  sub(".*\r \r ETYMOLOGY.\r \r","",chapters[[1]][1])
chapters = chapters[[1]][1:137]
#add a graph of words per chapter
word_frequencies = function(word,chapters){
freq_vec = c()
for (i in 1:length(chapters)){
all_words_c <- chapters[i] %>% str_split(",|\\.|[:space:]")
all_words_c <- all_words_c[[1]]
words_c <- all_words_c[all_words_c!=""]
total_words_chap = length(words_c)
word_freq = sort(table(words_c), decreasing = TRUE)
our_word = as.vector(word_freq[word])
if (is.na(our_word)){
freq_vec = c(freq_vec,0)}
else {
freq_vec = c(freq_vec,our_word/total_words_chap)}
}
return(freq_vec)
}
word_frequencies("Ahab",chapters)
word_frequencies("Moby",chapters)
word_frequencies("sea",chapters)
#add ggplot of the three words freq = y and x =chapter number
all_word_freq = table(words)
formula_function = function(all_word_freq){
all_word_freq <-((all_word_freq)/length(words))^2
return(sum(all_word_freq))
}
sum_freq = formula_function(all_word_freq)
set.seed(1)
simulation_func = function(words){
B <- 100000
counter = 0
for (i in 1:B){
sample_words <- sample(words,2,replace = TRUE)
if(sample_words[1] == sample_words[2]){
counter = counter + 1
}
}
return(counter/B)
}
AB_pro =  simulation_func(words)
AB_pro
unique_words = unique(words)
1/length(unique_words)
mobybook_clean = str_replace_all(mobybook, "[^a-zA-Z]", " ")
mobybook_clean = tolower(mobybook_clean)
#All words in mobybook
mobybook_split = strsplit(mobybook_clean, " ")
count_letters <- nchar(mobybook_split[[1]] )
#list of all five-letter with duplicates
five_letters <- mobybook_split[[1]][count_letters ==5]
five_unique = unique(five_letters)
#top 10 most frequent five-letter words with their frequencies
sort(table(five_letters), decreasing = TRUE)[1:10]
letter_freq <- table(let = unlist(strsplit(five_letters,'')),pos = sequence(nchar(five_letters)))
heatmap(letter_freq, Colv = NA, Rowv = NA, scale = "column", xlab = "column", ylab = "letter", main = "Letters Heatmap")
#Creating table of probability's according to each letter
alphabet <- letters[1:26]
letter_freq_un <- table(let = unlist(strsplit(unique(five_letters),'')),pos = sequence(nchar(unique(five_letters))))
letter_freq_un2 = letter_freq_un/(length(unique(five_letters)))
#help function in order to multiply a vector
multiply <- function(vec){
out <- 1
for(i in 1:length(vec)){
out <- out*vec[i]
}
out
}
probabilities = function (tablegiven,array) {
vec_prob_let <- c()
final_pro <- c()
for (word in array){
for (j in 1:5){
letter = substr(word,j,j)
#print(letter)
index_row<- match(letter,alphabet)
#print(index_row)
pij<- letter_freq_un2[index_row,j]
#print(pij)
vec_prob_let<- c(vec_prob_let,pij)
#print(vec_prob_let)
#print(probability)
#print(final_pro)
}
probability <- multiply(vec_prob_let)
final_pro <- c(final_pro,probability)
}
freq_prb_table = as.table(setNames(final_pro,array))
return (freq_prb_table)
}
#word_array = c("where","there")
#frequency_five = probabilities(letter_freq_un2,word_array)
frequency_five = probabilities(letter_freq_un2,unique(five_letters))
sort(frequency_five, decreasing = TRUE)[1:10]
letter_freq_un <- table(let = unlist(strsplit(unique(five_letters),'')),pos = sequence(nchar(unique(five_letters))))
letter_freq_un2 = letter_freq_un/(length(unique(five_letters)))
#vdnvdbskbtrsjh
common <- read.csv("five-letter-words-knuth.txt")
common <-as.vector(common)
list_string = function(list){
vec_string <- c()
for (i in 1:length(common[[1]])){
vec_string <-c(vec_string,common[[1]][i])}
return(vec_string)
}
words_string = list_string(common)
letter_freq_dict <- table(let = unlist(strsplit(words_string,'')),pos = sequence(nchar(words_string)))
heatmap(letter_freq_dict, Colv = NA, Rowv = NA, scale = "column", xlab = "column", ylab = "letter", main = "Letters Heatmap")
#help function for dictionary
data_dict2 = function(dictionary){
words_vec = dictionary
df <- data.frame(words_vec)
df[c(1:5)]<- str_split_fixed(df$words_vec,"",5)
df <- df[c(1:5)]
return(df)
}
Wordle = function(geuss,matching,dictionary){
dictionary = data_dict2(dictionary)
for (i in 1:length(geuss)){
for (j in 1:5){
if (matching[j + (i-1) *5] == 0) {
geuss_word <- as.vector(str_split_fixed(geuss[i], pattern = "", n = nchar(geuss[i])))
dictionary <- subset(dictionary, (dictionary[1]!=geuss_word[j]) &
(dictionary[2]!=geuss_word[j]) &
(dictionary[3]!=geuss_word[j]) &
(dictionary[4]!=geuss_word[j]) &
(dictionary[5]!=geuss_word[j]))
} else if (matching[j + (i-1) *5] == -1){
geuss_word <- as.vector(str_split_fixed(geuss[i], pattern = "", n = nchar(geuss[i])))
dictionary <- subset(dictionary, dictionary[1]==geuss_word[j] |
dictionary[2]==geuss_word[j] |
dictionary[3]==geuss_word[j] |
dictionary[4]==geuss_word[j] |
dictionary[5]==geuss_word[j])
dictionary <- subset(dictionary,dictionary[j]!=geuss_word[j])
} else {
geuss_word <- as.vector(str_split_fixed(geuss[i], pattern = "", n = nchar(geuss[i])))
dictionary <- subset(dictionary,dictionary[j]==geuss_word[j])
}
}
}
dictionary <- na.omit(dictionary)
df <- c(dictionary, sep = "")
result <- do.call(paste,df)
return(result)
}
geuss = c("south", "north")
#dictionary =c("bouth","turth","apple","mousy","fouls","black","green")
check = Wordle(geuss,c(c(-1, 1, 1, 0, 0), c(0, 1, 0, 0, 0)),words_string)
check
set.seed(1)
strategy_one = function(unknown){
# splitting unknown word into 5 letters
unknown_word = as.vector(str_split_fixed(unknown, pattern = "", n = nchar(unknown)))
# guessing 5 random letters
guesses = c(sample(letters, 1),sample(letters, 1),sample(letters, 1),sample(letters, 1),sample(letters, 1))
guess_count = 1
while (!(identical(unknown_word,guesses)==TRUE)){
guess_array = c()
guess_count = guess_count + 1
for (i in 1:5){
if(isTRUE(unknown_word[i]==guesses[i])==TRUE){
guess_array[i] = 1
}
else{
guess_array[i] = 0
guesses[i] = sample(letters, 1)
}
guessed_word = str_c(guesses,collapse="")
guessed_array = str_c(guess_array,collapse="")
}
print(cbind(guessed_word,guessed_array))
}
return (guess_count)
}
strategy_one_try = strategy_one("mouse")
expec_func <- function(x){
p <- 1/26
n <- 5
expec = 0
for (i in 1:10000){
dist <- (1-(1-p)^i)^n - (1-(1-p)^(i-1))^n
expectation = dist*i
expec = expec + expectation}
print(expec)
}
expectation_X = expec_func(10000)
set.seed(1)
monte_carlo <-replicate(100,strategy_one(sample(words_string,1)))
mean(monte_carlo)
plot(ecdf(monte_carlo))
points <- seq(1:100)
#better plot??
# Helper function:
wordle_match <- function(guess, word)  # 1: correct location, -1: wrong location, 0: missing
{
L <- nchar(guess)
match <- rep(0, L)
for(i in 1:L)
{
if(grepl(substr(guess, i, i), word, fixed=TRUE))
{match[i] = -1}
if(substr(guess, i, i) == substr(word, i, i))
{      match[i] = 1}
}
return(match)
}
strat2 = function(word_list, unknown_word){
unknown_word_2 = as.vector(str_split_fixed(unknown_word, pattern = "", n = nchar(unknown_word_2)))
#using 4 in order to find freq of words_string and find the starting word
wordle_freq <- table(let = unlist(strsplit(word_list,'')),pos = sequence(nchar(word_list)))
wordle_prob = wordle_freq/(length(word_list))
frequency_words = probabilities(wordle_prob,word_list)
picked_word<- sort(frequency_words , decreasing = TRUE)[1]
picked_word <- names(picked_word)
new_df <- word_list
while (!identical(picked_word,unknown_word)){
vec_array = wordle_match(picked_word, unknown_word)
#Create new dictionary using 5b
new_df = Wordle(picked_word,vec_array,new_df)
#choosing new word
wordle_freq <- table(let = unlist(strsplit(new_df,'')),pos = sequence(nchar(new_df)))
wordle_prob = wordle_freq/(length(new_df))
frequency_words = probabilities(wordle_prob,new_df)
picked_word<- sort(frequency_words , decreasing = TRUE)[1]
picked_word <- names(picked_word)
}
return(picked_word)
}
unknown_word <- "mouse"
a_df <- strat2(words_string, unknown_word)
a_df
strat3 = function(word_list, unknown_word){
picked_word = sample(word_list,1)
#using 4 in order to find freq of words_string and find the starting word
new_df <- word_list
while (!identical(picked_word,unknown_word)){
vec_array = wordle_match(picked_word, unknown_word)
#Create new dictionary using 5b
new_df = Wordle(picked_word,vec_array,new_df)
#choosing new word
picked_word = sample(word_list,1)
}
return(picked_word)
}
unknown_word <- "mouse"
strat3(words_string, unknown_word)
chapters_split_into_words <- strsplit(chapters,split = "\\W+") # split all chapters into words
words_per_chapter <- lengths(chapters_split_into_words) # create the vector of words per chapter
chapters_nums <- append(c('start', 'Etymology', 'Extracts'),c(1:135))
chapters_nums <- append(chapters_nums, c('Epilogue', 'end'))# create the X labels
allDat <- data.frame("chapter_nums"= chapters_nums,"words_per_chapter" = words_per_chapter)  # create a dataframe from the two lists
chapters_split_into_words <- strsplit(chapters,split = "\\W+") # split all chapters into words
words_per_chapter <- lengths(chapters_split_into_words) # create the vector of words per chapter
chapters_nums <- append(c('start', 'Etymology', 'Extracts'),c(1:137)
chapters_nums <- append(chapters_nums, c('Epilogue', 'end'))# create the X labels
chapters_split_into_words
word_frequencies = function(word,chapters){
freq_vec = c()
for (i in 1:length(chapters)){
all_words_c <- chapters[i] %>% str_split(",|\\.|[:space:]")
all_words_c <- all_words_c[[1]]
words_c <- all_words_c[all_words_c!=""]
total_words_chap = length(words_c)
word_freq = sort(table(words_c), decreasing = TRUE)
our_word = as.vector(word_freq[word])
if (is.na(our_word)){
freq_vec = c(freq_vec,0)}
else {
freq_vec = c(freq_vec,our_word/total_words_chap)}
}
return(freq_vec)
}
word_frequencies("Ahab",chapters)
word_frequencies("Moby",chapters)
word_frequencies("sea",chapters)
#add ggplot of the three words freq = y and x =chapter number
chapters_split_into_words <- strsplit(chapters,split = "\\W+") # split all chapters into words
w[!is.na(w) & w != ""]})
chaps_count <- sapply(chapters,function(c){   #splitting each chapter to words
length(
sapply(strsplit(c ,split = "\r \r \r \r \r "),function(w){
w[!is.na(w) & w != ""]})
chaps_count
knitr::opts_chunk$set(echo = TRUE)
chapters_nums
plot(chapters_nums,Ahab_freq)
Ahab_freq <- word_frequencies("Ahab",chapters)
plot(chapters_nums,Ahab_freq)
Ahab_freq
Ahab_freq <- word_frequencies("Ahab",chapters)
word_frequencies("Ahab",chapters)
library(tidyverse) # This includes dplyr, stringr, ggplot2, ..
library(data.table)
library(ggthemes)
library(stringr)
library(tidytext)
library(rvest)
options(scipen=alpha)
url <- 'https://www.gutenberg.org/files/2701/2701-h/2701-h.htm'
webpage <- read_html(url)
mobybook = html_text(webpage)
book <- webpage %>% html_nodes("div") %>% html_text()
book[1]
#lower_text <- str_to_lower(book, locale = "en")
all_words_html <- webpage %>% html_text() %>% str_split(",|\\.|[:space:]")
all_words_html <- all_words_html[[1]]
words <- all_words_html[all_words_html!=""]
print(length(words))
len_words <- str_length(words)
words_dist<- table(len_words)/length(words)
barplot(words_dist, main = "Distribution of word lenghts", xlab = "Lenght of word", ylab = "Distribution of word length")
#median
median(len_words)
#mean
mean(len_words)
#longest
max(len_words)
#most common
sort(table(words), decreasing = TRUE)[1]
sort(table(words), decreasing = TRUE)[1:10]
webpage <- read_html("https://www.gutenberg.org/files/2701/2701-h/2701-h.htm#link2HCH0004")
moby_lines <- html_text2(html_nodes(webpage, 'title,p,h1,h2,h3'))
mobybook <- paste(moby_lines, collapse = " ")
chapters = strsplit(mobybook,"\r \r \r \r \r ")
#etymology =
chapters[[1]][1] <-  sub(".*\r \r ETYMOLOGY.\r \r","",chapters[[1]][1])
chapters = chapters[[1]][1:137]
#add a graph of words per chapter
word_frequencies = function(word,chapters){
freq_vec = c()
for (i in 1:length(chapters)){
all_words_c <- chapters[i] %>% str_split(",|\\.|[:space:]")
all_words_c <- all_words_c[[1]]
words_c <- all_words_c[all_words_c!=""]
total_words_chap = length(words_c)
word_freq = sort(table(words_c), decreasing = TRUE)
our_word = as.vector(word_freq[word])
if (is.na(our_word)){
freq_vec = c(freq_vec,0)}
else {
freq_vec = c(freq_vec,our_word/total_words_chap)}
}
return(freq_vec)
}
Ahab_freq <- word_frequencies("Ahab",chapters)
Moby_freq <- word_frequencies("Moby",chapters)
Sea_freq <- word_frequencies("sea",chapters)
plot(chapters_nums,Ahab_freq)
Ahab_freq
chapters_nums
length(chapters_nums)
chapters_nums
length(chapters)
chapters
chapters_nums
chapter_nums[1]
chapters_nums[1]
chapters[[1]][1]
chapters_nums[1]
chapters[[1]][1]
chapters_nums[2]
chapters_nums[3]
word_frequencies = function(word,chapters){
freq_vec = c()
for (i in 1:length(chapters)){
all_words_c <- chapters[i] %>% str_split(",|\\.|[:space:]")
print(all_words_c)
}
word_amount = function(chapters){
freq_vec = c()
for (i in 1:length(chapters)){
all_words_c <- chapters[i] %>% str_split(",|\\.|[:space:]")
print(all_words_c)
}
num_word <- word_amount(chapters)
word_amount = function(chapters){
freq_vec = c()
for (i in 1:length(chapters)){
all_words_c <- chapters[i] %>% str_split(",|\\.|[:space:]")
}
return(all_words_c)
}
num_word <- word_amount(chapters)
num_word
word_amount = function(chapters){
for (i in 1:length(chapters)){
print(chapters[i])
all_words_c <- chapters[i] %>% str_split(",|\\.|[:space:]")
}
return(all_words_c)
}
num_word <- word_amount(chapters)
num_word
Ahab_freq <- word_frequencies("Ahab",chapters)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # This includes dplyr, stringr, ggplot2, ..
library(data.table)
library(ggthemes)
library(stringr)
library(tidytext)
library(rvest)
options(scipen=alpha)
url <- 'https://www.gutenberg.org/files/2701/2701-h/2701-h.htm'
webpage <- read_html(url)
#lower_text <- str_to_lower(book, locale = "en")
all_words_html <- webpage %>% html_text() %>% str_split(",|\\.|[:space:]")
len_words <- str_length(words)
words_dist<- table(len_words)/length(words)
barplot(words_dist, main = "Distribution of word lenghts", xlab = "Lenght of word", ylab = "Distribution of word length")
#median
median(len_words)
#mean
mean(len_words)
#longest
max(len_words)
#most common
sort(table(words), decreasing = TRUE)[1]
sort(table(words), decreasing = TRUE)[1:10]
webpage <- read_html("https://www.gutenberg.org/files/2701/2701-h/2701-h.htm#link2HCH0004")
webpage
webpage <- read_html("https://www.gutenberg.org/files/2701/2701-h/2701-h.htm#link2HCH0004")
webpage <- read_html("https://www.gutenberg.org/files/2701/2701-h/2701-h.htm#link2HCH0004")
url <- 'https://www.gutenberg.org/files/2701/2701-h/2701-h.htm'
webpage <- read_html(url)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # This includes dplyr, stringr, ggplot2, ..
library(data.table)
library(ggthemes)
library(stringr)
library(tidytext)
library(rvest)
options(scipen=alpha)
url <- 'https://www.gutenberg.org/files/2701/2701-h/2701-h.htm'
webpage <- read_html(url)
book <- webpage %>% html_nodes("div") %>% html_text()
