picked_word = sample(word_list,1)
counter = counter +1
if(if_print == TRUE){
print(picked_word)
}
}
print(counter)
}
unknown_word <- "mouse"
strat3(words_string, unknown_word,if_print = TRUE)
random_words = sample(words_string,100,replace = TRUE)
strat2_tries=sapply(random_words,function(new_word){strat2(words_string,new_word,if_print=FALSE)})
strat3_tries=sapply(random_words,function(new_word){strat3(words_string,new_word,if_print=FALSE)})
strat2_mean = mean(strat2_tries)
strat3_mean = mean(strat3_tries)
strat2_mean = mean(strat2_tries)
strat3_mean = mean(strat3_tries)
strat2_mean
strat3_mean
print(strat2_mean)
print(strat3_mean)
print(strat2_mean)
strat3_mean = mean(strat3_tries)
print(strat3_mean)
sort(table(words), decreasing = TRUE)
sort(table(words), decreasing = TRUE)[1:10]
sort(table(words), decreasing = TRUE)[1:10]
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # This includes dplyr, stringr, ggplot2, ..
library(data.table)
library(ggthemes)
library(stringr)
library(tidytext)
library(rvest)
options(scipen=alpha)
url <- 'https://www.gutenberg.org/files/2701/2701-h/2701-h.htm'
webpage <- read_html(url)
mobybook = html_text(webpage)
book <- webpage %>% html_nodes("div") %>% html_text()
book[1]
#lower_text <- str_to_lower(book, locale = "en")
all_words_html <- webpage %>% html_text() %>% str_split(",|\\.|[:space:]")
all_words_html <- all_words_html[[1]]
words <- all_words_html[all_words_html!=""]
print(length(words))
len_words <- str_length(words)
words_dist<- table(len_words)/length(words)
barplot(words_dist, main = "Distribution of word lenghts", xlab = "Lenght of word", ylab = "Distribution of word length")
#median
median(len_words)
#mean
mean(len_words)
#longest
max(len_words)
#most common
sort(table(words), decreasing = TRUE)[1]
sort(table(words), decreasing = TRUE)
sort(table(words), decreasing = TRUE)[1:10]
webpage <- read_html("https://www.gutenberg.org/files/2701/2701-h/2701-h.htm#link2HCH0004")
moby_lines <- html_text2(html_nodes(webpage, 'title,p,h1,h2,h3'))
mobybook <- paste(moby_lines, collapse = " ")
chapters = strsplit(mobybook,"\r \r \r \r \r ")
#etymology =
chapters[[1]][1] <-  sub(".*\r \r ETYMOLOGY.\r \r","",chapters[[1]][1])
chapters = chapters[[1]][1:137]
#Counting words per chapter
word_amount = function(chapters){
vec_words=vector(length=length(chapters))
for (i in 1:length(chapters)){
all_words_c <- chapters[i] %>% str_split(",|\\.|[:space:]")
words <- all_words_c[all_words_c!=""]
vec_words[i] = length(all_words_c[[1]])
}
return(vec_words)
}
#X-is vector
num_word <- word_amount(chapters)
chapter_index = c(1:137)
#chapter_index[1] = "Eti"
#chapter_index[137] = "Epi"
data.frame(num_word) %>% # plotting
ggplot(aes(x=chapter_index, y=num_word)) +
geom_point() +
geom_segment( aes(x=c(1:137), xend=c(1:137), y=0, yend=num_word))+
labs(x="Chapter Number",y= "Words Count Per Chapter",fill=c(1:137),title = "Words Count Per Chapter")
num_word <- word_amount(chapters)
chapter_index = c(1:137)
chapter_index[1] = "Eti"
chapter_index[137] = "Epi"
data.frame(num_word) %>% # plotting
ggplot(aes(x=chapter_index, y=num_word)) +
geom_point() +
geom_segment( aes(x=c(1:137), xend=c(1:137), y=0, yend=num_word))+
labs(x="Chapter Number",y= "Words Count Per Chapter",fill=c(1:137),title = "Words Count Per Chapter")
word_frequencies = function(word,chapters){
freq_vec = c()
for (i in 1:length(chapters)){
all_words_c <- chapters[i] %>% str_split(",|\\.|[:space:]")
all_words_c <- all_words_c[[1]]
words_c <- all_words_c[all_words_c!=""]
total_words_chap = length(words_c)
word_freq = sort(table(words_c), decreasing = TRUE)
our_word = as.vector(word_freq[word])
if (is.na(our_word)){
freq_vec = c(freq_vec,0)}
else {
freq_vec = c(freq_vec,our_word/total_words_chap)}
}
return(freq_vec)
}
Ahab_freq <- word_frequencies("Ahab",chapters)
Moby_freq <- word_frequencies("Moby",chapters)
Sea_freq <- word_frequencies("sea",chapters)
data.frame(Ahab_freq) %>% # plotting
ggplot(aes(x=chapter_index, y=Ahab_freq)) +
geom_point() +
geom_segment( aes(x=c(1:137), xend=c(1:137), y=0, yend=Ahab_freq))+
labs(x="Chapter Number",y= "Words Frequency Per Chapter",fill=c(1:137),title = "Words Count Per Chapter For Ahab")
data.frame(Moby_freq) %>% # plotting
ggplot(aes(x=chapter_index, y=Moby_freq)) +
geom_point() +
geom_segment( aes(x=c(1:137), xend=c(1:137), y=0, yend=Moby_freq))+
labs(x="Chapter Number",y= "Words Frequency Per Chapter",fill=c(1:137),title = "Words Count Per Chapter For Moby")
data.frame(Sea_freq) %>% # plotting
ggplot(aes(x=chapter_index, y=Sea_freq)) +
geom_point() +
geom_segment( aes(x=c(1:137), xend=c(1:137), y=0, yend=Sea_freq))+
labs(x="Chapter Number",y= "Words Frequency Per Chapter",fill=c(1:137),title = "Words Count Per Chapter For sea")
all_word_freq = table(words)
#this function calculates the probability of choosing the same word with a given frequency table
formula_function = function(all_word_freq){
all_word_freq <-((all_word_freq)/length(words))^2
#The probability of choosing the same word from the book:
return(sum(all_word_freq))
}
formula_function(all_word_freq)
set.seed(1)
simulation_func = function(words){
B <- 100000
counter = 0
for (i in 1:B){
sample_words <- sample(words,2,replace = TRUE)
if(sample_words[1] == sample_words[2]){
counter = counter + 1
}
}
return(counter/B)
}
simulation_func(words)
unique_words = unique(words)
1/length(unique_words)
mobybook_clean = str_replace_all(mobybook, "[^a-zA-Z]", " ")
mobybook_clean = tolower(mobybook_clean)
#All words in mobybook
mobybook_split = strsplit(mobybook_clean, " ")
count_letters <- nchar(mobybook_split[[1]] )
#list of all five-letter with duplicates
five_letters <- mobybook_split[[1]][count_letters ==5]
#amount of 5 letter words in the book:
print(length(five_letters))
five_unique = unique(five_letters)
#amount of 5 unique letter words in the book:
print(length(five_unique))
#top 10 most frequent five-letter words with their frequencies
sort(table(five_letters), decreasing = TRUE)[1:10]
letter_freq <- table(let = unlist(strsplit(five_letters,'')),pos = sequence(nchar(five_letters)))
heatmap(letter_freq, Colv = NA, Rowv = NA, scale = "column", xlab = "column", ylab = "letter", main = "Letters Heatmap")
#Creating table of probability's according to each letter
alphabet <- letters[1:26]
letter_freq_un <- table(let = unlist(strsplit(unique(five_letters),'')),pos = sequence(nchar(unique(five_letters))))
letter_freq_un2 = letter_freq_un/(length(unique(five_letters)))
#help function in order to multiply a vector
multiply <- function(vec){
out <- 1
for(i in 1:length(vec)){
out <- out*vec[i]
}
out
}
probabilities = function(tablegiven,array) {
vec_prob_let <- c()
final_pro <- c()
for (word in array){
for (j in 1:5){
letter = substr(word,j,j)
#print(letter)
index_row<- match(letter,alphabet)
#print(index_row)
pij<- letter_freq_un2[index_row,j]
#print(pij)
vec_prob_let<- c(vec_prob_let,pij)
#print(vec_prob_let)
#print(probability)
#print(final_pro)
}
probability <- multiply(vec_prob_let)
final_pro <- c(final_pro,probability)
}
freq_prb_table = as.table(setNames(final_pro,array))
return (freq_prb_table)
}
#word_array = c("where","there")
#frequency_five = probabilities(letter_freq_un2,word_array)
frequency_five = probabilities(letter_freq_un2,unique(five_letters))
sort(frequency_five, decreasing = TRUE)[1:10]
letter_freq_un <- table(let = unlist(strsplit(unique(five_letters),'')),pos = sequence(nchar(unique(five_letters))))
letter_freq_un2 = letter_freq_un/(length(unique(five_letters)))
common <- read.csv("five-letter-words-knuth.txt")
common <-as.vector(common)
#Converting a list to string
list_string = function(list){
vec_string <- c()
for (i in 1:length(common[[1]])){
vec_string <-c(vec_string,common[[1]][i])}
return(vec_string)
}
words_string = list_string(common)
letter_freq_dict <- table(let = unlist(strsplit(words_string,'')),pos = sequence(nchar(words_string)))
heatmap(letter_freq_dict, Colv = NA, Rowv = NA, scale = "column", xlab = "column", ylab = "letter", main = "Letters Heatmap")
#help function for dictionary- make in to a data frame
data_dict2 = function(dictionary){
words_vec = dictionary
df <- data.frame(words_vec)
df[c(1:5)]<- str_split_fixed(df$words_vec,"",5)
df <- df[c(1:5)]
return(df)
}
Wordle = function(geuss,matching,dictionary){
dictionary = data_dict2(dictionary)
for (i in 1:length(geuss)){
for (j in 1:5){
if (matching[j + (i-1) *5] == 0) {
geuss_word <- as.vector(str_split_fixed(geuss[i], pattern = "", n = nchar(geuss[i])))
dictionary <- subset(dictionary, (dictionary[1]!=geuss_word[j]) &
(dictionary[2]!=geuss_word[j]) &
(dictionary[3]!=geuss_word[j]) &
(dictionary[4]!=geuss_word[j]) &
(dictionary[5]!=geuss_word[j]))
} else if (matching[j + (i-1) *5] == -1){
geuss_word <- as.vector(str_split_fixed(geuss[i], pattern = "", n = nchar(geuss[i])))
dictionary <- subset(dictionary, dictionary[1]==geuss_word[j] |
dictionary[2]==geuss_word[j] |
dictionary[3]==geuss_word[j] |
dictionary[4]==geuss_word[j] |
dictionary[5]==geuss_word[j])
dictionary <- subset(dictionary,dictionary[j]!=geuss_word[j])
} else {
geuss_word <- as.vector(str_split_fixed(geuss[i], pattern = "", n = nchar(geuss[i])))
dictionary <- subset(dictionary,dictionary[j]==geuss_word[j])
}
}
}
dictionary <- na.omit(dictionary)
df <- c(dictionary, sep = "")
result <- do.call(paste,df)
return(result)
}
geuss = c("south", "north")
check = Wordle(geuss,c(c(-1, 1, 1, 0, 0), c(0, 1, 0, 0, 0)),words_string)
check
set.seed(1)
strategy_one = function(unknown,tn=TRUE){
# splitting unknown word into 5 letters
unknown_word = as.vector(str_split_fixed(unknown, pattern = "", n = nchar(unknown)))
# guessing 5 random letters
guesses = c(sample(letters, 1),sample(letters, 1),sample(letters, 1),sample(letters, 1),sample(letters, 1))
#counter for amount of times the function runs until correct word is guessed
guess_count = 1
while (!(identical(unknown_word,guesses)==TRUE)){
guess_array = c()
guess_count = guess_count + 1
for (i in 1:5){
if(isTRUE(unknown_word[i]==guesses[i])==TRUE){
guess_array[i] = 1
}
else{
guess_array[i] = 0
guesses[i] = sample(letters, 1)
}
guessed_word = str_c(guesses,collapse="")
guessed_array = str_c(guess_array,collapse="")
}
if (tn==TRUE){
print(cbind(guessed_word,guessed_array))
}  }
return (guess_count)
}
strategy_one_try = strategy_one("mouse")
expec_func <- function(x){
p <- 1/26
n <- 5
expec = 0
for (i in 1:10000){
dist <- (1-(1-p)^i)^n - (1-(1-p)^(i-1))^n
expectation = dist*i
expec = expec + expectation}
print(expec)
}
expectation_X = expec_func(10000)
set.seed(1)
sampled_word = sample(words_string,1)
monte_carlo <-replicate(100,strategy_one(sampled_word,tn=FALSE))
mean(monte_carlo)
plot(ecdf(monte_carlo))
points <- seq(1:100)
# Helper function:
wordle_match <- function(guess, word)  # 1: correct location, -1: wrong location, 0: missing
{
L <- nchar(guess)
match <- rep(0, L)
for(i in 1:L)
{
if(grepl(substr(guess, i, i), word, fixed=TRUE))
{match[i] = -1}
if(substr(guess, i, i) == substr(word, i, i))
{      match[i] = 1}
}
return(match)
}
plot(ecdf(strat2_tries))
plot(ecdf(strat3_tries))
print(mean(strat2_tries))
print(mean(strat3_tries))
plot(ecdf(strat2_tries))
plot(ecdf(strat3_tries))
sample_data <- data.frame(x = c(strat2_tries,strat3_tries),
group = gl(2, 1000))
ggplot(sample_data, aes(x=x, col=group)) +
stat_ecdf()
d.f <- data.frame(
grp = as.factor( rep( c("strategy 2","strategy 3"), each=40 ) ) ,
val = c( sample(c(2:4,6:8,12),40,replace=TRUE), sample(1:4,40,replace=TRUE) )
)
d.f <- arrange(d.f,grp,val)
d.f.ecdf <- ddply(d.f, .(grp), transform, ecdf=ecdf(val)(val) )
sample_data <- data.frame(x = c(strat2_tries,strat3_tries),
group = gl(2, 1000))
ggplot(sample_data, aes(x=x, col=group)) +
stat_ecdf()
plot(ecdf(strat2_tries))
plot(ecdf(strat3_tries))
sort(frequency_five, decreasing = TRUE)[1:10]
#Creating table of probability's according to each letter
alphabet <- letters[1:26]
letter_freq_un <- table(let = unlist(strsplit(unique(five_letters),'')),pos = sequence(nchar(unique(five_letters))))
letter_freq_un2 = letter_freq_un/(length(unique(five_letters)))
#help function in order to multiply a vector
multiply <- function(vec){
out <- 1
for(i in 1:length(vec)){
out <- out*vec[i]
}
out
}
probabilities = function(tablegiven,array) {
vec_prob_let <- c()
final_pro <- c()
for (word in array){
for (j in 1:5){
letter = substr(word,j,j)
index_row<- match(letter,alphabet)
pij<- letter_freq_un2[index_row,j]
vec_prob_let<- c(vec_prob_let,pij)
}
probability <- multiply(vec_prob_let)
final_pro <- c(final_pro,probability)
}
freq_prb_table = as.table(setNames(final_pro,array))
return (freq_prb_table)
}
frequency_five = probabilities(letter_freq_un2,unique(five_letters))
sort(frequency_five, decreasing = TRUE)[1:10]
letter_freq_un <- table(let = unlist(strsplit(unique(five_letters),'')),pos = sequence(nchar(unique(five_letters))))
letter_freq_un2 = letter_freq_un/(length(unique(five_letters)))
letter_freq_un2
letter_freq_dict
heatmap(letter_freq_dict, Colv = NA, Rowv = NA, scale = "column", xlab = "column", ylab = "letter", main = "Letters Heatmap")
# Helper function:
wordle_match <- function(guess, word)  # 1: correct location, -1: wrong location, 0: missing
{
L <- nchar(guess)
match <- rep(0, L)
for(i in 1:L)
{
if(grepl(substr(guess, i, i), word, fixed=TRUE))
{match[i] = -1}
if(substr(guess, i, i) == substr(word, i, i))
{      match[i] = 1}
}
return(match)
}
plot(ecdf(strat2_tries))
plot(ecdf(strat3_tries))
print(mean(strat2_tries))
print(mean(strat3_tries))
plot(ecdf(strat2_tries))
plot(ecdf(strat3_tries))
mobybook_split
word_amount = function(chapters){
vec_words=vector(length=length(chapters))
for (i in 1:length(chapters)){
all_words_c <- chapters[i] %>% str_split(",|\\.|[:space:]")
words <- all_words_c[all_words_c!=""]
vec_words[i] = length(words[[1]])
}
return(vec_words)
}
word_amount(chapters)
word_amount = function(chapters){
vec_words=vector(length=length(chapters))
for (i in 1:length(chapters)){
all_words_c <- chapters[i] %>% str_split(",|\\.|[:space:]")
words <- all_words_c[all_words_c!=""]
print(words)
vec_words[i] = length(words[[1]])
}
return(vec_words)
}
word_amount(chapters)
word_amount = function(chapters){
vec_words=vector(length=length(chapters))
for (i in 1:length(chapters)){
all_words_c <- chapters[i] %>% str_split(",|\\.|[:space:]")
words <- all_words_c[all_words_c!=""]
print(class(words))
vec_words[i] = length(words[[1]])
}
return(vec_words)
}
word_amount(chapters)
chapters[[1]][1]
word_amount = function(chapters){
vec_words=vector(length=length(chapters))
for (i in 1:length(chapters)){
all_words_c <- chapters[i] %>% str_split(",|\\.|[:space:]")
all_words_c = lapply(all_words_c,function(z){z[!is.na(z) & z != ""]})
vec_words[i] = length(all_words_c[[1]])
}
return(vec_words)
}
word_amount(chapters)
all_words_c <- chapters[1] %>% str_split(",|\\.|[:space:]")
all_words_c = lapply(all_words_c,function(z){z[!is.na(z) & z != ""]})
all_words_c
webpage <- read_html("https://www.gutenberg.org/files/2701/2701-h/2701-h.htm#link2HCH0004")
moby_lines <- html_text2(html_nodes(webpage, 'title,p,h1,h2,h3'))
mobybook <- paste(moby_lines, collapse = " ")
chapters = strsplit(mobybook,"\r \r \r \r \r ")
#etymology =
chapters[[1]][1] <-  sub(".*\r \r ETYMOLOGY.\r \r","",chapters[[1]][1])
chapters = chapters[[1]][1:137]
all_words_c <- chapters[1] %>% str_split(",|\\.|[:space:]")
#Counting words per chapter
word_amount = function(chapters){
vec_words=vector(length=length(chapters))
for (i in 1:length(chapters)){
all_words_c <- chapters[i] %>% str_split(",|\\.|[:space:]")
all_words_c = lapply(all_words_c,function(z){z[!is.na(z) & z != ""]})
vec_words[i] = length(all_words_c[[1]])
}
return(vec_words)
}
#X-is vector
num_word <- word_amount(chapters)
chapter_index = c(1:137)
chapter_index[1] = "Eti"
chapter_index[137] = "Epi"
#plotting
data.frame(num_word) %>%
ggplot(aes(x=chapter_index, y=num_word)) +
geom_point() +
geom_segment( aes(x=c(1:137), xend=c(1:137), y=0, yend=num_word))+
labs(x="Chapter Number",y= "Words Count Per Chapter",fill=c(1:137),title = "Words Count Per Chapter")
word_frequencies = function(word,chapters){
freq_vec = c()
for (i in 1:length(chapters)){
all_words_c <- chapters[i] %>% str_split(",|\\.|[:space:]")
all_words_c <- all_words_c[[1]]
words_c <- all_words_c[all_words_c!=""]
total_words_chap = length(words_c)
word_freq = sort(table(words_c), decreasing = TRUE)
our_word = as.vector(word_freq[word])
if (is.na(our_word)){
freq_vec = c(freq_vec,0)}
else {
freq_vec = c(freq_vec,our_word/total_words_chap)}
}
return(freq_vec)
}
Ahab_freq <- word_frequencies("Ahab",chapters)
Moby_freq <- word_frequencies("Moby",chapters)
Sea_freq <- word_frequencies("sea",chapters)
# plotting Ahab
data.frame(Ahab_freq) %>%
ggplot(aes(x=chapter_index, y=Ahab_freq)) +
geom_point() +
geom_segment( aes(x=c(1:137), xend=c(1:137), y=0, yend=Ahab_freq))+
labs(x="Chapter Number",y= "Words Frequency Per Chapter",fill=c(1:137),title = "Words Count Per Chapter For Ahab")
# plotting Moby
data.frame(Moby_freq) %>%
ggplot(aes(x=chapter_index, y=Moby_freq)) +
geom_point() +
geom_segment( aes(x=c(1:137), xend=c(1:137), y=0, yend=Moby_freq))+
labs(x="Chapter Number",y= "Words Frequency Per Chapter",fill=c(1:137),title = "Words Count Per Chapter For Moby")
# plotting Sea
data.frame(Sea_freq) %>%
ggplot(aes(x=chapter_index, y=Sea_freq)) +
geom_point() +
geom_segment( aes(x=c(1:137), xend=c(1:137), y=0, yend=Sea_freq))+
labs(x="Chapter Number",y= "Words Frequency Per Chapter",fill=c(1:137),title = "Words Count Per Chapter For sea")
letter_freq_dict
letter_freq
